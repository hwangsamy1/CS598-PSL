{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e44e5a-5703-4f9d-a6b3-6730a0028345",
   "metadata": {},
   "source": [
    "# PSL Coding Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72febb8e-a01f-4e9e-8b8e-45072301b8fb",
   "metadata": {},
   "source": [
    "Members: Amy Hwang, Christian Tam, Monil Kaneria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c7afd-3f85-436b-8dc1-1c2d04319060",
   "metadata": {},
   "source": [
    "Contributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a72f672e-6b42-4e5e-9d99-314f14d58255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdeb07-4fb2-4816-83cb-00130fd96307",
   "metadata": {},
   "source": [
    "## Part 1: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa87137-3c3d-4edd-a38f-d76eed634ee0",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f07c1dd-7918-4a4b-9602-cbe942a2777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return an n-by-G matrix, where the (i, j)th entry is the conditional probability P(Zi = k | xi). \n",
    "# i ranges from 1 to n and k ranges from 1 to G.\n",
    "\n",
    "def eStep(sigma, G, p, x, mu):\n",
    "    U, D, UT = np.linalg.svd(sigma)\n",
    "    dBar = np.diag(1.0 / np.sqrt(D))\n",
    "\n",
    "    xBar = x @ UT @ dBar\n",
    "    muBar = mu @ UT @ dBar\n",
    "\n",
    "    diff = xBar[:, np.newaxis, :] - muBar\n",
    "    distances = np.sum(diff ** 2, axis=2)\n",
    "\n",
    "    probs = np.exp(distances * -0.5)\n",
    "    probs *= p\n",
    "    probs = (probs / probs.sum(axis=1, keepdims=True))\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e69a977d-c98e-4ddb-9dbc-8df33e6087c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Return the updated parameters for the Gaussian mixture model.\n",
    "\n",
    "# Input:\n",
    "#    data: nxp matrix\n",
    "#    mu_k: mean vector for component k\n",
    "#    p: nx1 vector, the probability of each sample belonging to the kth component\n",
    "\n",
    "def mStep(data, probs, G):\n",
    "    n, d = data.shape\n",
    "    sigma_new = np.zeros((G, d, d))\n",
    "\n",
    "    weighted_sums = probs.sum(axis=0)\n",
    "    p_new = weighted_sums / n\n",
    "    mu_new = (probs.T @ data) / weighted_sums[:, np.newaxis]\n",
    "    weighted_cov = np.zeros((d, d))\n",
    "    for k in range(G):\n",
    "        x_centered = data - mu_new[k]\n",
    "        weighted_cov += (probs[:, k][:, np.newaxis] * x_centered).T @ x_centered\n",
    "    sigma_new = weighted_cov / np.sum(weighted_sums)\n",
    "\n",
    "    return p_new, mu_new, sigma_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ae6c911-bd85-42ac-ab85-8e6940b21f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Computes the log-likelihood of the data given the parameters.\n",
    "# Input:\n",
    "#    data: nxp matrix\n",
    "#    mu: mean vector for component k\n",
    "#    p: nx1 vector, the probability of each sample belonging to the kth component\n",
    "\n",
    "def logik(data, G, sigma, mu, p):\n",
    "    density = np.zeros((np.shape(data)[0], G))\n",
    "    \n",
    "    # Calculate the multivariable normal pdf\n",
    "    for k in range(G):\n",
    "        # calculate mahalanobis distance between each data point and mean of component k\n",
    "        mu_k = mu[k]\n",
    "        data_mu = data - mu_k\n",
    "        inv_cov = np.linalg.inv(sigma)\n",
    "        left = np.dot(data_mu, inv_cov)\n",
    "        mahal = np.dot(left, data_mu.T)\n",
    "        mahal_distance = mahal.diagonal()\n",
    "\n",
    "        # Use mahalanobis distance and the determinant of the covariance matrix of component k to get the multivar normal pdf.\n",
    "        normal_pdf = np.exp(-0.5 * mahal_distance) / (2 * math.pi * np.linalg.det(sigma))\n",
    "\n",
    "        # Multiply the pdf by the mixture weight p_k to get the probability density of the data point under component k.\n",
    "        density[:, k] = p[k] @ normal_pdf\n",
    "\n",
    "    # Get the log of the sum of the probability densities across all components\n",
    "    log_row_sums = np.log(np.sum(density, axis=1))\n",
    "\n",
    "    # Sum the log-likelihoods of all data points to get the total log-likelihood.\n",
    "    log_likelihood = np.sum(log_row_sums)\n",
    "    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbede400-7370-4ff8-a61c-f2ce715f4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function. Call the Estep and Mstep functions. Returns the estimated parameters and log-likelihood (via\n",
    "# the loglik function)\n",
    "#\n",
    "# Input:\n",
    "#   data: the dataset.\n",
    "#   G: The number of components.\n",
    "#   params: Initial parameters.\n",
    "#   itmax: The number of iterations.\n",
    "# Output:\n",
    "#   prob: A G-dimensional probability vector (p1,…,pG)\n",
    "#   mean: A p-by-G matrix with the k-th column being μk, the p-dimensional mean for the k-th Gaussian component.\n",
    "#   Sigma: A p-by-p covariance matrix Σ shared by all G components\n",
    "\n",
    "def myEM(data, G, sigma_init, mu_init, p_init, itmax):\n",
    "    sigma = sigma_init\n",
    "    mu = mu_init\n",
    "    p = p_init\n",
    "    pi = np.zeros((G))\n",
    "    li_threshold = 1e-3\n",
    "    li_previous = 0\n",
    "    li_current = 1\n",
    "    loop_count = 0\n",
    "\n",
    "    for i in range(itmax):\n",
    "    # while abs(li_current - li_previous) > li_threshold or loop_count < itmax:\n",
    "        # Call Estep to get the updated probability matrix\n",
    "        probs = eStep(sigma, G, p, data, mu)\n",
    "\n",
    "        # Call Mstep to get the updated parameters\n",
    "        p, mu, sigma = mStep(data, probs, G)\n",
    "\n",
    "        # Call logik to get the log-likelihood of the data given the updated parameters\n",
    "        # li_current = loglik(sigma, G, p, data, mu)\n",
    "            \n",
    "    return p, mu.T, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cbb33-c66e-4290-9460-244bcf73ef91",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80a908b7-321a-458d-b0b3-70e65cd1146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('faithful.dat') as faithful:\n",
    "    for row in faithful:\n",
    "        data.append(row.split()[1:])\n",
    "        \n",
    "    # convert to float values\n",
    "    data = np.array([[float(value) for value in row] for row in data[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bd851-35da-4cf6-9ea2-eb682d7b1c6c",
   "metadata": {},
   "source": [
    "#### Case 1: G = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ded403d9-e28d-4015-844c-5c73980d09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 2\n",
    "itmax = 20\n",
    "n = len(data)\n",
    "p1 = 10 / n\n",
    "p2 = 1 - p1\n",
    "p_init = [p1, p2]\n",
    "\n",
    "cluster_1 = data[:10]\n",
    "cluster_2 = data[10:]\n",
    "mu1 = np.mean(cluster_1, axis=0)\n",
    "mu2 = np.mean(cluster_2, axis=0)\n",
    "mu_init = np.array([mu1, mu2])\n",
    "\n",
    "cov_matrix_1 = np.sum([(i - mu1).reshape(-1, 1) @ (i - mu1).reshape(-1, 1).T for i in cluster_1], axis=0)\n",
    "cov_matrix_2 = np.sum([(i - mu2).reshape(-1, 1) @ (i - mu2).reshape(-1, 1).T for i in cluster_2], axis=0)\n",
    "# 2x2 matrix\n",
    "cov_matrix_init = (cov_matrix_1 + cov_matrix_2) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7787e8e-ceff-4511-b952-f4e1b0db1114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob\n",
      "[0.04363422 0.07718656 0.87917922]\n",
      "mean\n",
      "[[ 3.51006918  2.81616674  3.54564083]\n",
      " [77.10563811 63.35752634 71.25084801]]\n",
      "Sigma\n",
      "[[  1.26015772  13.51153756]\n",
      " [ 13.51153756 177.96419105]]\n"
     ]
    }
   ],
   "source": [
    "prob, mean, Sigma = myEM(data, G, cov_matrix_init, mu_init, p_init, itmax)\n",
    "print(\"prob\")\n",
    "print(prob)\n",
    "\n",
    "print(\"mean\")\n",
    "print(mean)\n",
    "\n",
    "print(\"Sigma\")\n",
    "print(Sigma)\n",
    "\n",
    "# print(\"Prob: \" + prob + \"\\nMean: \" + mean + \"\\nSigma: \" + Sigma + \"\\nlogik_val: \" + logik_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa8d21-d6f6-41dd-93d0-ee39011eaf18",
   "metadata": {},
   "source": [
    "#### Case 2: G = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e30de900-8362-41ff-8f64-c5af8f44ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "itmax = 20\n",
    "x = data\n",
    "n = len(x)\n",
    "p1 = 10 / n\n",
    "p2 = 20 / n\n",
    "p3 = 1 - p1 - p2\n",
    "p_init = [p1, p2, p3]\n",
    "\n",
    "cluster_1 = x[:10]\n",
    "cluster_2 = x[10:30]\n",
    "cluster_3 = x[30:]\n",
    "mu1 = np.mean(cluster_1, axis=0)\n",
    "mu2 = np.mean(cluster_2, axis=0)\n",
    "mu3 = np.mean(cluster_3, axis=0)\n",
    "mu_init = np.array([mu1, mu2, mu3])\n",
    "\n",
    "cov_matrix_1 = np.sum([(i - mu1).reshape(-1, 1) @ (i - mu1).reshape(-1, 1).T for i in cluster_1], axis=0)\n",
    "cov_matrix_2 = np.sum([(i - mu2).reshape(-1, 1) @ (i - mu2).reshape(-1, 1).T for i in cluster_2], axis=0)\n",
    "cov_matrix_3 = np.sum([(i - mu3).reshape(-1, 1) @ (i - mu3).reshape(-1, 1).T for i in cluster_3], axis=0)\n",
    "\n",
    "# 2x2 matrix\n",
    "cov_matrix_init = (cov_matrix_1 + cov_matrix_2 + cov_matrix_3) / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b90a8ce8-3b0f-4990-b088-f5018470aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob\n",
      "[0.04363422 0.07718656 0.87917922]\n",
      "mean\n",
      "[[ 3.51006918  2.81616674  3.54564083]\n",
      " [77.10563811 63.35752634 71.25084801]]\n",
      "Sigma\n",
      "[[  1.26015772  13.51153756]\n",
      " [ 13.51153756 177.96419105]]\n"
     ]
    }
   ],
   "source": [
    "prob, mean, Sigma = myEM(data, G, cov_matrix_init, mu_init, p_init, itmax)\n",
    "\n",
    "print(\"prob\")\n",
    "print(prob)\n",
    "\n",
    "print(\"mean\")\n",
    "print(mean)\n",
    "\n",
    "print(\"Sigma\")\n",
    "print(Sigma)\n",
    "\n",
    "# print(\"Prob: \" + prob + \"\\nMean: \" + mean + \"\\nSigma: \" + Sigma + \"\\nlogik_val: \" + logik_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732882c-42ef-4e60-9d02-8312ca5948be",
   "metadata": {},
   "source": [
    "## Part 2: HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a82adc-b047-4a21-887b-eaa3c72a2699",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e036d20c-8eea-436b-9aab-f2c1609f9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward algorithm to calculate alpha values\n",
    "def forward_algorithm(data, w, A, B):\n",
    "    T = len(data)\n",
    "    alpha = np.zeros((T, mz))\n",
    "    \n",
    "    # Initialization step\n",
    "    alpha[0, :] = w * B[:, data[0]]\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, T):\n",
    "        for i in range(mz):\n",
    "            alpha[t, i] = np.sum(alpha[t - 1] * A[:, i]) * B[i, data[t]]\n",
    "            \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de281053-8a86-4a96-aa57-d559d0c431b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward algorithm to calculate beta values\n",
    "def backward_algorithm(data, A, B):\n",
    "    T = len(data)\n",
    "    mz = A.shape[0]\n",
    "    beta = np.zeros((T, mz))\n",
    "\n",
    "    # Initialization step\n",
    "    beta[T - 1, :] = 1\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(mz):\n",
    "            beta[t, i] = np.sum(A[i, :] * B[:, data[t + 1]] * beta[t + 1, :])\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e556e6bc-cf05-4201-92d5-ac4f9670bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baum-Welch one step (E-step and M-step)\n",
    "def BW_onestep(data, w, A, B, mz, mx):\n",
    "    T = len(data)\n",
    "    \n",
    "    # E-step\n",
    "    alpha = forward_algorithm(data, w, A, B)\n",
    "    beta = backward_algorithm(data, A, B)\n",
    "    gamma = np.zeros((T, mz))\n",
    "    xi = np.zeros((T - 1, mz, mz))\n",
    "\n",
    "    for t in range(T):\n",
    "        gamma[t, :] = alpha[t, :] * beta[t, :]\n",
    "        gamma[t, :] /= np.sum(gamma[t, :])\n",
    "\n",
    "    for t in range(T - 1):\n",
    "        denom = np.sum(alpha[t, :] * np.sum(A * B[:, data[t + 1]].reshape(1, -1) * beta[t + 1, :], axis=1))\n",
    "        for i in range(mz):\n",
    "            xi[t, i, :] = alpha[t, i] * A[i, :] * B[:, data[t + 1]] * beta[t + 1, :]\n",
    "            xi[t, i, :] /= denom\n",
    "\n",
    "    # M-step\n",
    "    A_new = np.sum(xi, axis=0) / np.sum(gamma[:-1, :], axis=0).reshape(-1, 1)\n",
    "    B_new = np.zeros((mz, mx))\n",
    "\n",
    "    for i in range(mz):\n",
    "        for l in range(mx):\n",
    "            B_new[i, l] = np.sum(gamma[data == l, i])\n",
    "        B_new[i, :] /= np.sum(gamma[:, i])\n",
    "\n",
    "    return A_new, B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6939d429-01d3-4b24-b6f0-9f5e23df146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myBW(data, w_init, A_init, B_init, mz_init, mx_init, iteration):\n",
    "    # Example usage with provided data\n",
    "    mz = mz_init\n",
    "    mx = mx_init\n",
    "    w = w_init\n",
    "    A = A_init\n",
    "    B_new = B_init\n",
    "\n",
    "    for _ in range(iteration):\n",
    "        A, B_new = BW_onestep(data, w, A, B_new, mz, mx)\n",
    "    \n",
    "    return A, B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bb2e70e-1f8b-4803-8f18-67dfcf9bb246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi algorithm to find the most likely sequence of latent states\n",
    "def myViterbi(data, w, A, B, mz, mx):\n",
    "    T = len(data)\n",
    "    mz = len(w)\n",
    "    delta = np.zeros((T, mz))\n",
    "    psi = np.zeros((T, mz), dtype=int)\n",
    "\n",
    "    # Initialization\n",
    "    delta[0, :] = w * B[:, data[0]]\n",
    "    psi[0, :] = 0\n",
    "\n",
    "    # Recursion\n",
    "    for t in range(1, T):\n",
    "        for i in range(mz):\n",
    "            delta[t, i] = np.max(delta[t - 1] * A[:, i]) * B[i, data[t]]\n",
    "            psi[t, i] = np.argmax(delta[t - 1] * A[:, i])\n",
    "\n",
    "    # Termination\n",
    "    Z = np.zeros(T, dtype=int)\n",
    "    Z[T - 1] = np.argmax(delta[T - 1, :]) + 1\n",
    "\n",
    "    # Path backtracking\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        Z[t] = psi[t + 1, Z[t + 1] - 1] + 1\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6baebbf-14ca-4cae-85db-31b04c1a056e",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f1fb2-0cc1-45e9-97c7-353850c33729",
   "metadata": {},
   "source": [
    "#### Part 1: Testing Baum-Welch and Viterbi with Coding4_part2_data.txt where B = [[1/9, 3/9, 5/9], [1/6, 2/6, 3/6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "538b4900-5bf0-4522-9e22-be048b77e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('Coding4_part2_data.txt', dtype=int) - 1\n",
    "mz = 2  \n",
    "mx = 3  \n",
    "w = np.array([0.5, 0.5])\n",
    "A = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "B = np.array([[1/9, 3/9, 5/9], [1/6, 2/6, 3/6]])\n",
    "iteration = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06209ae2-4f3d-4c2d-926a-bd7b0e26ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A: \n",
      "[[0.49793938 0.50206062]\n",
      " [0.44883431 0.55116569]]\n",
      "\n",
      "Emission matrix B: \n",
      "[[0.22159897 0.20266127 0.57573976]\n",
      " [0.34175148 0.17866665 0.47958186]]\n",
      "\n",
      "Most likely latent sequence Z: \n",
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Baum-Welch algorithm\n",
    "A_updated, B_updated = myBW(data, w, A, B, mz, mx, iteration)\n",
    "print(f\"Transition matrix A: \\n{A_updated}\") \n",
    "print(f\"\\nEmission matrix B: \\n{B_updated}\")\n",
    "\n",
    "# Viterbi algorithm to get the most likely sequence of hidden states\n",
    "Z = myViterbi(data, w, A_updated, B_updated, mz, mx)\n",
    "print(f\"\\nMost likely latent sequence Z: \\n{Z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06351b16-e630-4ecd-8478-5638e18cbec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is our Viterbi implementation equal to the benchmark?: True\n"
     ]
    }
   ],
   "source": [
    "# Checking our Viterbi algorithm implementation with the given benchmark in Coding4_part2_Z.txt\n",
    "viterbi_benchmark = []\n",
    "\n",
    "with open('Coding4_part2_Z.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        viterbi_benchmark.extend(map(int, line.split()))\n",
    "\n",
    "viterbi_benchmark = np.asarray(viterbi_benchmark)\n",
    "\n",
    "are_equal = np.array_equal(Z, viterbi_benchmark)\n",
    "print(\"Is our Viterbi implementation equal to the benchmark?: \" + str(are_equal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c68f2-5306-4b0d-b67e-32bcdd92b9aa",
   "metadata": {},
   "source": [
    "#### Part 2: Testing Baum-Welch and Viterbi with Coding4_part2_data.txt where B = [[1/3, 1/3, 1/3], [1/3, 1/3, 1/3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70025c0d-3c3a-4d82-9275-a68d9cba1dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A after 20 iterations with uniform B initialization: \n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "Emission matrix B after 20 iterations with uniform B initialization: \n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n",
      "\n",
      "Transition matrix A after 100 iterations with uniform B initialization: \n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "Emission matrix B after 100 iterations with uniform B initialization: \n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n"
     ]
    }
   ],
   "source": [
    "mz = 2\n",
    "mx = 3\n",
    "w = np.array([0.5, 0.5])\n",
    "A = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "B = np.array([[1/3, 1/3, 1/3], [1/3, 1/3, 1/3]])\n",
    "\n",
    "# Run Baum-Welch algorithm for 20 iterations\n",
    "iteration = 20\n",
    "A_20, B_20 = myBW(data, w, A, B, mz, mx, iteration)\n",
    "print(f\"Transition matrix A after 20 iterations with uniform B initialization: \\n{A_20}\") \n",
    "print(f\"\\nEmission matrix B after 20 iterations with uniform B initialization: \\n{B_20}\\n\")\n",
    "\n",
    "# Run Baum-Welch algorithm for 100 iterations\n",
    "iteration = 100\n",
    "A_100, B_100 = myBW(data, w, A, B, mz, mx, iteration)\n",
    "print(f\"Transition matrix A after 100 iterations with uniform B initialization: \\n{A_100}\") \n",
    "print(f\"\\nEmission matrix B after 100 iterations with uniform B initialization: \\n{B_100}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd581f4-a9df-421d-a3ff-81d7fe5ba800",
   "metadata": {},
   "source": [
    "#### Explain why the resulting A and B matrices had these outcomes. You should understand why we cannot initialize our parameters in a way that makes the latent states indistinguishable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff02e217-e4e5-46f6-92e9-2d0777dff2c5",
   "metadata": {},
   "source": [
    "Poor initial probability values in a Hidden Markov Model which cause indistinguishable latent states impact the overall model performance. If the emission probabilities for different hidden states are too similar or incorrect, the model will struggle to distinguish between the states. There would be slow convergence and/or the model would settle into a local minima as a result. Comparing the 20th and 100th iterations we can see that there have been no change in the inital probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee83c0b-4bbb-4478-8470-8eeb7518cd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
