{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b082e5bb-a77f-494f-b127-2b1ae453bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import dateutil\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ecee48-08a8-44fa-a1ec-04c130761601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fbe8363d-0999-412a-b849-243de4574c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What have we tried (III)\n",
    "\n",
    "def svd(train_data, d = 5):\n",
    "    # some random value\n",
    "    final_data = []\n",
    "    departments = train_data['Dept'].unique()\n",
    "\n",
    "    for dept in departments:\n",
    "        # Grabbing stores that have the current dept \n",
    "        filtered_train = train_data[train_data['Dept'] == dept]\n",
    "        \n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        train_dept_ts = selected_columns.pivot(index='Store', columns='Date', values='Weekly_Sales').reset_index()\n",
    "\n",
    "        # Replace all missing values with zero\n",
    "        X_train = train_dept_ts.iloc[:, 1:]\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_train = np.nan_to_num(X_train)\n",
    "\n",
    "        # Center X values\n",
    "        store_mean = np.mean(X_train, axis=0)\n",
    "        X_centered = X_train - store_mean\n",
    "\n",
    "        # Implement SVD\n",
    "        U, D, VT = np.linalg.svd(X_centered, full_matrices=False)\n",
    "        \n",
    "        # Take first d components and fill rest of the diag with 0s\n",
    "        D[d:] = 0\n",
    "\n",
    "        # Make a reduced rank (smoothed) version of the original dataset\n",
    "        X_bar = (U @ np.diag(D) @ VT) + store_mean\n",
    "\n",
    "        # need to add some logic to rebuild the train data with column labels and such\n",
    "        # concat it into final_data\n",
    "        reconstructed = pd.DataFrame(X_bar, columns=train_dept_ts.columns[1:], index=train_dept_ts['Store'])\n",
    "        reconstructed = reconstructed.reset_index().melt(id_vars=['Store'], var_name='Date', value_name='Weekly_Sales')\n",
    "        reconstructed['Dept'] = dept\n",
    "\n",
    "        final_data.append(reconstructed)\n",
    "    \n",
    "    return pd.concat(final_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f94aa0ed-2b46-429a-8486-b51910d2c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(data):\n",
    "    shift = 1\n",
    "    threshold = 1.1\n",
    "    final_data = data\n",
    "    departments = data['Dept'].unique()\n",
    "\n",
    "    for dept in departments:\n",
    "        # Grabbing stores that have the current dept \n",
    "        filtered_train = data[data['Dept'] == dept]\n",
    "        \n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Pred']]\n",
    "        print(selected_columns)\n",
    "        print(selected_columns.shape)\n",
    "        data_filtered = selected_columns.pivot(index='Store', columns='Date', values='Weekly_Pred').reset_index()\n",
    "        #print(data_filtered.shape)\n",
    "        \n",
    "        #week_1 = '2011-12-02'\n",
    "        #week_5 = '2011-12-30'\n",
    "        #fold_5 = train_dept_ts.loc[:, (train_dept_ts.columns >= week_1) & (train_dept_ts.columns <= week_5)]\n",
    "        #result = apply_shift(data_filtered, shift, threshold)\n",
    "        #result = apply_shift(selected_columns, shift, threshold)\n",
    "        \n",
    "        reconstructed = pd.DataFrame(result, columns=data_filtered.columns[1:], index=data_filtered['Store'])\n",
    "        reconstructed = reconstructed.reset_index().melt(id_vars=['Store'], var_name='Date', value_name='Weekly_Pred')\n",
    "        print(reconstructed)\n",
    "        print(final_data[final_data['Dept'] == dept])\n",
    "        final_data[final_data['Dept'] == dept][['Weekly_Pred']] = reconstructed\n",
    "        \n",
    "        reconstructed = reconstructed.reset_index().melt(id_vars=['Store'], var_name='Date', value_name='Weekly_Pred')\n",
    "        reconstructed['Dept'] = dept\n",
    "        #print(reconstructed)\n",
    "\n",
    "        #final_data.append(reconstructed)\n",
    "        pred_d_index = data.where(data['Dept'] == dept)\n",
    "        #final_data[[week_1, week_2, week_3, week_4, week_5]] = \n",
    "        \n",
    "    print(final_data)\n",
    "    final = pd.concat(final_data, ignore_index=True)\n",
    "    #print(final)\n",
    "    return pd.concat(final_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "552594ec-e1b1-4cb0-ad3c-2ab71880d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shift(data, shift=1, threshold=1.1):\n",
    "    week_1 = '2011-12-02'\n",
    "    week_2 = '2011-12-09'\n",
    "    week_3 = '2011-12-16'\n",
    "    week_4 = '2011-12-23'\n",
    "    week_5 = '2011-12-30'\n",
    "    \n",
    "    fold_5 = data.loc[:, (data.columns >= week_1) & (data.columns <= week_5)]\n",
    "    \n",
    "    if week_1 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_1] = np.full(rows, np.nan)\n",
    "    \n",
    "    if week_2 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_2] = np.full(rows, np.nan)\n",
    "    \n",
    "    if week_3 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_3] = np.full(rows, np.nan)\n",
    "   \n",
    "    if week_4 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_4] = np.full(rows, np.nan)\n",
    "    if week_5 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_5] = np.full(rows, np.nan)\n",
    "                                 \n",
    "    baseline = fold_5[[week_1, week_5]].mean(axis=1).mean()\n",
    "    surge = fold_5[[week_2, week_3, week_4]].mean(axis=1).mean()\n",
    "    \n",
    "    fold_5[fold_5.isna()] = 0\n",
    "    \n",
    "    if surge / baseline > threshold:\n",
    "        shifted_sales = ((7-shift)/7) * fold_5\n",
    "        shifted_sales[[week_2, week_3, week_4, week_5]] = shifted_sales[[week_2, week_3, week_4, week_5]].values + (shift/7)*fold_5[[week_1, week_2, week_3, week_4]].values\n",
    "        shifted_sales[[week_1]] = fold_5[[week_1]]\n",
    "        data.loc[:, (data.columns >= week_1) & (data.columns <= week_5)] = shifted_sales\n",
    "\n",
    "    print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e76eaec5-10b6-47c0-b830-ab09fe885e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shift2(data, shift=1, threshold=1.1):\n",
    "    week_1 = '2011-12-02'\n",
    "    week_2 = '2011-12-09'\n",
    "    week_3 = '2011-12-16'\n",
    "    week_4 = '2011-12-23'\n",
    "    week_5 = '2011-12-30'\n",
    "    \n",
    "    fold_5 = data.loc[:, (data.columns >= week_1) & (data.columns <= week_5)]\n",
    "    \n",
    "    if week_1 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_1] = np.full(rows, np.nan)\n",
    "    \n",
    "    if week_2 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_2] = np.full(rows, np.nan)\n",
    "    \n",
    "    if week_3 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_3] = np.full(rows, np.nan)\n",
    "   \n",
    "    if week_4 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_4] = np.full(rows, np.nan)\n",
    "    if week_5 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_5] = np.full(rows, np.nan)\n",
    "                                 \n",
    "    baseline = fold_5[[week_1, week_5]].mean(axis=1).mean()\n",
    "    surge = fold_5[[week_2, week_3, week_4]].mean(axis=1).mean()\n",
    "    \n",
    "    fold_5[fold_5.isna()] = 0\n",
    "    \n",
    "    if surge / baseline > threshold:\n",
    "        shifted_sales = ((7-shift)/7) * fold_5\n",
    "        shifted_sales[[week_2, week_3, week_4, week_5]] = shifted_sales[[week_2, week_3, week_4, week_5]].values + (shift/7)*fold_5[[week_1, week_2, week_3, week_4]].values\n",
    "        shifted_sales[[week_1]] = fold_5[[week_1]]\n",
    "        data.loc[:, (data.columns >= week_1) & (data.columns <= week_5)] = shifted_sales\n",
    "\n",
    "    print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc9125a-7768-4612-a1a9-f1899b772d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(train_file_path, test_file_path, pred_file_path):\n",
    "\n",
    "    # Load data\n",
    "    print(train_file_path)\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Check for date range in test data for fold 5\n",
    "    is_fold_5 = test['Date'].between(\"2011-11-04\", \"2011-12-30\").any()\n",
    "\n",
    "    # Apply SVD by department for dimensionality reduction\n",
    "    train_svd = svd(train, 8)\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    # Filter for shared store-dept pairs in train and test; filter out pairs with zero occurrences\n",
    "    train_pairs = train_svd[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "\n",
    "    # Join with common pairs and add week/year columns\n",
    "    train_split = unique_pairs.merge(train_svd, on=['Store', 'Dept'], how='left')\n",
    "    train_split = preprocess(train_split)\n",
    "\n",
    "    # set up data for each split\n",
    "    X = patsy.dmatrix('Weekly_Sales + Store + Dept + Yr  + Wk',\n",
    "                      data = train_split,\n",
    "                      return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "    \n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    X = patsy.dmatrix('Store + Dept + Yr  + Wk', \n",
    "                        data = test_split, \n",
    "                        return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "    \n",
    "    keys = list(train_split)\n",
    "    \n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "     \n",
    "        Y = X_train['Weekly_Sales']\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "        \n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "     \n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "    \n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-16:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                \n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "    \n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "        \n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    # Post prediction adjustment for fold_5\n",
    "    postprocess(test_pred)\n",
    "    \n",
    "    # Join predictions with original test data\n",
    "    result = test.merge(test_pred, on=[\"Store\", \"Dept\", \"Date\"], how=\"left\")\n",
    "\n",
    "    # Handle missing predictions and round results\n",
    "    result[\"Weekly_Pred\"] = result[\"Weekly_Pred\"].fillna(0)\n",
    "\n",
    "    # Ensure we have all these colummns\n",
    "    result = result[[\"Store\", \"Dept\", \"Date\", \"IsHoliday\", \"Weekly_Pred\"]]\n",
    "\n",
    "    # Export results\n",
    "    result.to_csv(pred_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5c55c109-3dce-48f6-b1cd-1883fde33145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Proj2_Data/fold_5/train.csv\n",
      "       Store        Date   Weekly_Pred\n",
      "0        1.0  2011-11-04  32210.419086\n",
      "1        1.0  2011-11-11  17401.258168\n",
      "2        1.0  2011-11-18  18002.777556\n",
      "3        1.0  2011-11-25  17087.499597\n",
      "4        1.0  2011-12-02  22316.094321\n",
      "...      ...         ...           ...\n",
      "26227   45.0  2011-12-02  19822.847722\n",
      "26228   45.0  2011-12-09  28072.788667\n",
      "26229   45.0  2011-12-16  37455.669106\n",
      "26230   45.0  2011-12-23  51646.761194\n",
      "26231   45.0  2011-12-30  12276.460665\n",
      "\n",
      "[405 rows x 3 columns]\n",
      "(405, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Proj2_Data/fold_5/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m pred_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Proj2_Data/fold_5/mypred.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_file_path\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[9], line 75\u001b[0m, in \u001b[0;36mprocess\u001b[1;34m(train_file_path, test_file_path, pred_file_path)\u001b[0m\n\u001b[0;32m     72\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_pred, tmp_pred], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Post prediction adjustment for fold_5\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Join predictions with original test data\u001b[39;00m\n\u001b[0;32m     78\u001b[0m result \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mmerge(test_pred, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDept\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[130], line 23\u001b[0m, in \u001b[0;36mpostprocess\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     14\u001b[0m data_filtered \u001b[38;5;241m=\u001b[39m selected_columns\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStore\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekly_Pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#print(data_filtered.shape)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#week_1 = '2011-12-02'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#result = apply_shift(data_filtered, shift, threshold)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#result = apply_shift(selected_columns, shift, threshold)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mresult\u001b[49m, columns\u001b[38;5;241m=\u001b[39mdata_filtered\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:], index\u001b[38;5;241m=\u001b[39mdata_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStore\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m reconstructed\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mmelt(id_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStore\u001b[39m\u001b[38;5;124m'\u001b[39m], var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekly_Pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(reconstructed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "train_file_path = f'./Proj2_Data/fold_5/train.csv'\n",
    "test_file_path = f'./Proj2_Data/fold_5/test.csv'\n",
    "pred_file_path = f'./Proj2_Data/fold_5/mypred.csv'\n",
    "\n",
    "process(train_file_path, test_file_path, pred_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14055c-da59-4c13-93b0-b1c297dc2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "\n",
    "for i in range(num_folds):\n",
    "    train_file_path = f'./Proj2_Data/fold_{i+1}/train.csv'\n",
    "    test_file_path = f'./Proj2_Data/fold_{i+1}/test.csv'\n",
    "    pred_file_path = f'./Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "\n",
    "    print(\"fold \", i)\n",
    "    process(train_file_path, test_file_path, pred_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a7b86-cbaf-4ec3-a141-e2d09483536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval():\n",
    "    test_with_label = pd.read_csv('./Proj2_Data/test_with_label.csv')\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'./Proj2_Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "\n",
    "        file_path = f'./Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "        test_pred = test_pred.drop(columns=['IsHoliday'])\n",
    "\n",
    "        new_test = test.merge(test_pred, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c32967-c679-4ec0-89ba-9a7774820e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "wae = myeval()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c82e5f-aba7-4837-8589-a68216e6df1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
