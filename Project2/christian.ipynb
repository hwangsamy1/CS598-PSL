{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b082e5bb-a77f-494f-b127-2b1ae453bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import dateutil\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "import warnings\n",
    "import time\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ecee48-08a8-44fa-a1ec-04c130761601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe8363d-0999-412a-b849-243de4574c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What have we tried (III)\n",
    "\n",
    "def svd(train_data, d = 5):\n",
    "    # some random value\n",
    "    final_data = []\n",
    "    departments = train_data['Dept'].unique()\n",
    "\n",
    "    for dept in departments:\n",
    "        # Grabbing stores that have the current dept \n",
    "        filtered_train = train_data[train_data['Dept'] == dept]\n",
    "        \n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        train_dept_ts = selected_columns.pivot(index='Store', columns='Date', values='Weekly_Sales').reset_index()\n",
    "\n",
    "        # Replace all missing values with zero\n",
    "        X_train = train_dept_ts.iloc[:, 1:]\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_train = np.nan_to_num(X_train)\n",
    "\n",
    "        # Center X values\n",
    "        store_mean = np.mean(X_train, axis=0)\n",
    "        X_centered = X_train - store_mean\n",
    "\n",
    "        # Implement SVD\n",
    "        U, D, VT = np.linalg.svd(X_centered, full_matrices=False)\n",
    "        \n",
    "        # Take first d components and fill rest of the diag with 0s\n",
    "        D[d:] = 0\n",
    "\n",
    "        # Make a reduced rank (smoothed) version of the original dataset\n",
    "        X_bar = (U @ np.diag(D) @ VT) + store_mean\n",
    "\n",
    "        # need to add some logic to rebuild the train data with column labels and such\n",
    "        # concat it into final_data\n",
    "        reconstructed = pd.DataFrame(X_bar, columns=train_dept_ts.columns[1:], index=train_dept_ts['Store'])\n",
    "        reconstructed = reconstructed.reset_index().melt(id_vars=['Store'], var_name='Date', value_name='Weekly_Sales')\n",
    "        reconstructed['Dept'] = dept\n",
    "\n",
    "        final_data.append(reconstructed)\n",
    "    \n",
    "    return pd.concat(final_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94aa0ed-2b46-429a-8486-b51910d2c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(data):\n",
    "    shift = 1\n",
    "    threshold = 1.1\n",
    "    final_data = data\n",
    "    departments = data['Dept'].unique()\n",
    "\n",
    "    for dept in departments:\n",
    "        # Grabbing stores that have the current dept \n",
    "        filtered_train = data[data['Dept'] == dept]\n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Pred']]\n",
    "        data_filtered = selected_columns.pivot(index='Store', columns='Date', values='Weekly_Pred').reset_index()\n",
    "        \n",
    "        result = apply_shift(data_filtered, shift, threshold).set_index('Store')\n",
    "        \n",
    "        reconstructed = pd.DataFrame(result, columns=data_filtered.columns[1:], index=data_filtered['Store'])\n",
    "        reconstructed = reconstructed.reset_index().melt(id_vars=['Store'], var_name='Date', value_name='Weekly_Pred')\n",
    "        reconstructed['Dept'] = dept\n",
    "\n",
    "        for _, row in reconstructed.iterrows():\n",
    "            final_data.loc[(final_data['Store'] == row['Store']) & (final_data['Date'] == row['Date']) & (final_data['Dept'] == row['Dept']), 'Weekly_Pred'] = row['Weekly_Pred']\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76eaec5-10b6-47c0-b830-ab09fe885e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shift(data, shift=1, threshold=1.1):\n",
    "    week_1 = '2011-12-02'\n",
    "    week_2 = '2011-12-09'\n",
    "    week_3 = '2011-12-16'\n",
    "    week_4 = '2011-12-23'\n",
    "    week_5 = '2011-12-30'\n",
    "    \n",
    "    fold_5 = data.loc[:, (data.columns >= week_1) & (data.columns <= week_5)]\n",
    "    \n",
    "    if week_1 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_1] = np.full(rows, np.nan)\n",
    "    \n",
    "    if week_2 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_2] = np.full(rows, np.nan)\n",
    "    \n",
    "    if week_3 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_3] = np.full(rows, np.nan)\n",
    "   \n",
    "    if week_4 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_4] = np.full(rows, np.nan)\n",
    "    if week_5 not in fold_5:\n",
    "        rows = fold_5.shape[0]\n",
    "        fold_5[week_5] = np.full(rows, np.nan)\n",
    "                                 \n",
    "    baseline = fold_5[[week_1, week_5]].mean(axis=1).mean()\n",
    "    surge = fold_5[[week_2, week_3, week_4]].mean(axis=1).mean()\n",
    "    \n",
    "    fold_5[fold_5.isna()] = 0\n",
    "    \n",
    "    if surge / baseline > threshold:\n",
    "        shifted_sales = ((7-shift)/7) * fold_5\n",
    "        shifted_sales[[week_2, week_3, week_4, week_5]] = shifted_sales[[week_2, week_3, week_4, week_5]].values + (shift/7)*fold_5[[week_1, week_2, week_3, week_4]].values\n",
    "        shifted_sales[[week_1]] = fold_5[[week_1]]\n",
    "        data.loc[:, (data.columns >= week_1) & (data.columns <= week_5)] = shifted_sales\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc9125a-7768-4612-a1a9-f1899b772d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(train_file_path = 'train.csv', test_file_path = 'test.csv', pred_file_path = 'mypred.csv'):\n",
    "    # Load data\n",
    "    print(train_file_path)\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "\n",
    "    # Check for date range in test data for fold 5\n",
    "    is_fold_5 = test['Date'].between(\"2011-11-04\", \"2011-12-30\").any()\n",
    "    \n",
    "    # Apply SVD by department for dimensionality reduction\n",
    "    train_svd = svd(train, 8)\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    # Filter for shared store-dept pairs in train and test; filter out pairs with zero occurrences\n",
    "    train_pairs = train_svd[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "\n",
    "    # Join with common pairs and add week/year columns\n",
    "    train_split = unique_pairs.merge(train_svd, on=['Store', 'Dept'], how='left')\n",
    "    train_split = preprocess(train_split)\n",
    "\n",
    "    # set up data for each split\n",
    "    X = patsy.dmatrix('Weekly_Sales + Store + Dept + Yr  + Wk',\n",
    "                      data = train_split,\n",
    "                      return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "    \n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    X = patsy.dmatrix('Store + Dept + Yr  + Wk', \n",
    "                        data = test_split, \n",
    "                        return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "    \n",
    "    keys = list(train_split)\n",
    "    \n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "     \n",
    "        Y = X_train['Weekly_Sales']\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "        \n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "     \n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "    \n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-16:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                \n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "    \n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "        \n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    # Post prediction adjustment for fold_5\n",
    "    if is_fold_5:\n",
    "        test_pred = postprocess(test_pred)\n",
    "    \n",
    "    # Join predictions with original test data\n",
    "    result = test.merge(test_pred, on=[\"Store\", \"Dept\", \"Date\"], how=\"left\")\n",
    "\n",
    "    # Handle missing predictions and round results\n",
    "    result[\"Weekly_Pred\"] = result[\"Weekly_Pred\"].fillna(0)\n",
    "\n",
    "    # Ensure we have all these colummns\n",
    "    result = result[[\"Store\", \"Dept\", \"Date\", \"IsHoliday\", \"Weekly_Pred\"]]\n",
    "\n",
    "    # Export results\n",
    "    result.to_csv(pred_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe14055c-da59-4c13-93b0-b1c297dc2ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Proj2_Data/fold_1/train.csv\n",
      "Execution time: 38.56968855857849 seconds\n",
      "./Proj2_Data/fold_2/train.csv\n",
      "Execution time: 41.057206869125366 seconds\n",
      "./Proj2_Data/fold_3/train.csv\n",
      "Execution time: 42.65760135650635 seconds\n",
      "./Proj2_Data/fold_4/train.csv\n",
      "Execution time: 43.63476252555847 seconds\n",
      "./Proj2_Data/fold_5/train.csv\n",
      "Execution time: 105.15479183197021 seconds\n",
      "./Proj2_Data/fold_6/train.csv\n",
      "Execution time: 44.49999189376831 seconds\n",
      "./Proj2_Data/fold_7/train.csv\n",
      "Execution time: 44.59416174888611 seconds\n",
      "./Proj2_Data/fold_8/train.csv\n",
      "Execution time: 46.106958866119385 seconds\n",
      "./Proj2_Data/fold_9/train.csv\n",
      "Execution time: 47.019410133361816 seconds\n",
      "./Proj2_Data/fold_10/train.csv\n",
      "Execution time: 46.65165734291077 seconds\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "\n",
    "for i in range(num_folds):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_file_path = f'./Proj2_Data/fold_{i+1}/train.csv'\n",
    "    test_file_path = f'./Proj2_Data/fold_{i+1}/test.csv'\n",
    "    pred_file_path = f'./Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "\n",
    "    #print(\"fold \", i)\n",
    "    process(train_file_path, test_file_path, pred_file_path) \n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1a7b86-cbaf-4ec3-a141-e2d09483536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval():\n",
    "    test_with_label = pd.read_csv('./Proj2_Data/test_with_label.csv')\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'./Proj2_Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "\n",
    "        file_path = f'./Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "        test_pred = test_pred.drop(columns=['IsHoliday'])\n",
    "\n",
    "        new_test = test.merge(test_pred, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c32967-c679-4ec0-89ba-9a7774820e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1950.380\n",
      "\t1375.480\n",
      "\t1394.559\n",
      "\t1540.027\n",
      "\t2030.375\n",
      "\t1640.754\n",
      "\t1692.460\n",
      "\t1412.618\n",
      "\t1423.198\n",
      "\t1439.092\n",
      "1589.894\n"
     ]
    }
   ],
   "source": [
    "wae = myeval()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
