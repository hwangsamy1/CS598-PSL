{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7466626b-ba12-47ef-8243-8c7e351d0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371aa5c4-76fb-4a3f-9396-5451c7e4d379",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7aea3-55d5-4a2a-a0c8-62b036d17634",
   "metadata": {},
   "source": [
    "Attempting GridSearchCV for hyperparameter selection. The penalty is elasticnet, and saga is the solver. We used a max iteration count of 2000 to ensure convergence due to the large dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45c20289-2953-40c9-a97e-be4d1e45e73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Fold  0\n",
      "Best hyperparameters: {'C': 10, 'l1_ratio': 0.1, 'max_iter': 2000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.94784\n",
      "Test set accuracy: 0.94772\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Fold  1\n",
      "Best hyperparameters: {'C': 10, 'l1_ratio': 0.1, 'max_iter': 2000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.94784\n",
      "Test set accuracy: 0.94772\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Fold  2\n",
      "Best hyperparameters: {'C': 10, 'l1_ratio': 0.1, 'max_iter': 2000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.94784\n",
      "Test set accuracy: 0.94772\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Fold  3\n",
      "Best hyperparameters: {'C': 10, 'l1_ratio': 0.1, 'max_iter': 2000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.94784\n",
      "Test set accuracy: 0.94772\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Fold  4\n",
      "Best hyperparameters: {'C': 10, 'l1_ratio': 0.1, 'max_iter': 2000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.94784\n",
      "Test set accuracy: 0.94772\n"
     ]
    }
   ],
   "source": [
    "num_splits = 5\n",
    "cv_k = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1, 3, 5, 10],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'solver': ['saga'],\n",
    "    'l1_ratio': [0.1, 0.5],\n",
    "    'max_iter': [2000],\n",
    "}\n",
    "\n",
    "for i in range(num_splits):\n",
    "    start_time = time.time()\n",
    "    train_file_path = f'./F24_Proj3_data/split_{2}/train.csv'\n",
    "    test_file_path = f'./F24_Proj3_data/split_{2}/test.csv'\n",
    "    test_y_file_path = f'./F24_Proj3_data/split_{2}/test_y.csv'\n",
    "\n",
    "    # Load data\n",
    "    X_train = pd.read_csv(train_file_path).iloc[:, 3:]\n",
    "    y_train = pd.read_csv(train_file_path).iloc[:, 1]\n",
    "\n",
    "    X_test = pd.read_csv(test_file_path).iloc[:, 2:]\n",
    "    y_test = pd.read_csv(test_y_file_path).iloc[:, 1]\n",
    "\n",
    "    log_reg = LogisticRegression()\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=log_reg, \n",
    "                               param_grid = param_grid,\n",
    "                               cv=cv_k,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1,\n",
    "                               scoring='accuracy')\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nFold \", i+1) \n",
    "    # Access the best parameters and best estimator\n",
    "    print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions with the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test set accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d3307-d2f1-4b97-905e-ff5bb44c46b2",
   "metadata": {},
   "source": [
    "Now we'll train the logistic regression model using the best C and l1 ratio we found earlier. C = 10, l1 = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f34504f-01c6-45fd-b899-92b931019850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1: AUC Score for LogisticRegression: 0.9870942\n",
      "| Execution time: 30.1842 seconds\n",
      "Split 2: AUC Score for LogisticRegression: 0.9867907\n",
      "| Execution time: 30.1352 seconds\n",
      "Split 3: AUC Score for LogisticRegression: 0.9864187\n",
      "| Execution time: 30.8156 seconds\n",
      "Split 4: AUC Score for LogisticRegression: 0.9869783\n",
      "| Execution time: 31.2468 seconds\n",
      "Split 5: AUC Score for LogisticRegression: 0.9862662\n",
      "| Execution time: 31.1201 seconds\n"
     ]
    }
   ],
   "source": [
    "num_splits = 5\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    l1_ratio=0.1,  \n",
    "    C=10,\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for i in range(num_splits):\n",
    "    start_time = time.time()\n",
    "    train_file_path = f'./F24_Proj3_data/split_{i+1}/train.csv'\n",
    "    test_file_path = f'./F24_Proj3_data/split_{i+1}/test.csv'\n",
    "    test_y_file_path = f'./F24_Proj3_data/split_{i+1}/test_y.csv'\n",
    "\n",
    "    # Load data\n",
    "    X_train = pd.read_csv(train_file_path).iloc[:, 3:]\n",
    "    y_train = pd.read_csv(train_file_path).iloc[:, 1]\n",
    "\n",
    "    X_test = pd.read_csv(test_file_path).iloc[:, 2:]\n",
    "    y_test = pd.read_csv(test_y_file_path).iloc[:, 1]\n",
    "\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f'Split {i+1}: AUC Score for LogisticRegression: {auc_score:.7f}')\n",
    "    print(f'| Execution time: {round(time.time() - start_time, 4)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a8501-cf90-48d8-bc1c-0f3f12e5e4a9",
   "metadata": {},
   "source": [
    "## Interpretability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef4fbf-6554-4985-8684-d2c243241258",
   "metadata": {},
   "source": [
    "Using split 1 and the corresponding trained model, implement an interpretability approach to identify which parts of each review have an impact on the sentiment prediction. Apply your method to 5 randomly selected positive reviews and 5 randomly selected negative reviews from the split 1 test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00da98ef-5be8-42f3-abef-8d7797dc5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = f'./F24_Proj3_data/split_1/train.csv'\n",
    "test_file_path = f'./F24_Proj3_data/split_1/test.csv'\n",
    "test_y_file_path = f'./F24_Proj3_data/split_1/test_y.csv'\n",
    "\n",
    "train = pd.read_csv(train_file_path)\n",
    "test = pd.read_csv(test_file_path)\n",
    "test_y = pd.read_csv(test_y_file_path)\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "             'you', 'your', 'yours', 'their', 'they', 'his', 'her', 'she',\n",
    "             'he', 'a', 'an', 'and', 'is', 'was', 'are', 'were', 'him',\n",
    "             'himself', 'has', 'have', 'it', 'its', 'the', 'us']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5282d0-4a43-431f-a934-8034fc969e47",
   "metadata": {},
   "source": [
    "We are doing some preprocessing:\n",
    "- Replacing HTML tags from reviews with a space character\n",
    "- Removing stop words\n",
    "- Convert to lowercase\n",
    "- remove rarely used words\n",
    "Note: The token pattern below treats words separated by apostrophes as a single token rather two splitting it into two tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c3f8ec-dd86-4039-8ed6-14f57d620b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review'] = train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "test['review'] = test['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "    stop_words=stop_words,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                        # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer\n",
    ")\n",
    "\n",
    "dtm_train = vectorizer.fit_transform(train['review'])\n",
    "# dtm_test = vectorizer.fit_transform(test['review'])\n",
    "\n",
    "# log_reg.fit(dtm_train, train['sentiment'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "# preds = log_reg.predict_proba(dtm_test)[:, 1]  # Get the probabilities for class 1\n",
    "\n",
    "# print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae16ee8-ee93-4213-82e5-33f476eafd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(dtm_train)\n",
    "\n",
    "lasso_model = Lasso(alpha=0.00623)\n",
    "\n",
    "lasso_model.fit(X_train_scaled, train['sentiment'])\n",
    "\n",
    "# Get the coefficients\n",
    "lasso_coefs = lasso_model.coef_\n",
    "print('Lasso Coefficients:', lasso_coefs.sum())\n",
    "\n",
    "# Create a DataFrame with feature names and their coefficients\n",
    "feature_coef_df = pd.DataFrame(\n",
    "    {'Feature': np.array(vectorizer.get_feature_names_out()),\n",
    "     'Coefficient': lasso_coefs})\n",
    "\n",
    "selected_features = feature_coef_df[feature_coef_df['Coefficient'] != 0]\n",
    "vocabulary = list(selected_features['Feature'].values)\n",
    "\n",
    "with open('./myvocab.txt', 'w') as file:\n",
    "    # Iterate through the list and write each word to a new line\n",
    "    for word in vocabulary:\n",
    "        file.write(word + \"\\n\")\n",
    "\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a82ab3-3ac4-432f-8c23-daf43a050338",
   "metadata": {},
   "source": [
    "Setting a seed for consistency in random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f597ae-6ac5-44ad-9918-f078d921700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96977890-2fa1-4fd6-b4ec-6d8544e43773",
   "metadata": {},
   "outputs": [],
   "source": [
    "myvocab = open(\"myvocab.txt\", \"r\").read().splitlines()\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=myvocab, ngram_range=(1, 4))\n",
    "dtm_test = tfidf_vectorizer.fit_transform(dtm_train['review'])\n",
    "# log_reg.fit(dtm_test, test_y['sentiment'])\n",
    "# print(myvocab)\n",
    "# print(test_y)\n",
    "# print(dtm_test)\n",
    "\n",
    "# Need to set a seed before selecting samples\n",
    "\n",
    "positive_reviews = test[test_y['sentiment'] == 1].sample(5, random_state=42)\n",
    "negative_reviews = test[test_y['sentiment'] == 0].sample(5, random_state=42)\n",
    "selected_reviews = pd.concat([positive_reviews, negative_reviews])\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=['Negative', 'Positive'])\n",
    "\n",
    "def explain_review(review_text):\n",
    "    print(review_text)\n",
    "    explanation = explainer.explain_instance(review_text, log_reg.predict_proba, num_features=10)\n",
    "    \n",
    "    explanation.show_in_notebook(text=True)\n",
    "\n",
    "for i, row in selected_reviews.iterrows():\n",
    "    explain_review(row['review'])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
