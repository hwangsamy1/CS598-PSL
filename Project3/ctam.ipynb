{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20df1c04-6eb4-41ca-b411-45fdf8323d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from lime) (3.9.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from lime) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from lime) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from lime) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/anaconda3/lib/python3.12/site-packages (from lime) (1.5.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from lime) (0.24.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (10.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->lime) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->lime) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->lime) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->lime) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=e80eb8d0f057bb352956594237aa9ee9239771fd8ed9a80dc0df213cca8085ad\n",
      "  Stored in directory: /Users/christiantam/Library/Caches/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
      "Successfully built lime\n",
      "Installing collected packages: lime\n",
      "Successfully installed lime-0.2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5fecc46-1d5b-47c9-b77d-4445d096dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, Lasso\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a6d2a-e10a-439f-97ff-a64440ab7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification\n",
    "\n",
    "num_splits = 5\n",
    "\n",
    "for i in range(num_splits):\n",
    "    start_time=time.time()\n",
    "    train_file_path = f'./F24_Proj3_data/split_{i+1}/train.csv'\n",
    "    test_file_path = f'./F24_Proj3_data/split_{i+1}/test.csv'\n",
    "    test_y_file_path = f'./F24_Proj3_data/split_{i+1}/test_y.csv'\n",
    "\n",
    "    # Load data\n",
    "    X_train = pd.read_csv(train_file_path).iloc[:, 3:]\n",
    "    y_train = pd.read_csv(train_file_path).iloc[:, 1]\n",
    "\n",
    "    X_test = pd.read_csv(test_file_path).iloc[:, 2:]\n",
    "    y_test = pd.read_csv(test_y_file_path).iloc[:, 1]\n",
    "\n",
    "    clf = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f'Split {i}: AUC Score for split {i}: {auc_score:.7f} | Execution time : {round(time.time() - start_time, 4)} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423f6e4-b107-45fb-800f-4da04b7b7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LogisticRegressionCV with elastic net penalty. Using cross validation and finding the best C and l1 ratio to speed up training.\n",
    "\n",
    "num_splits = 5\n",
    "cv = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for i in range(num_splits):\n",
    "    start_time = time.time()\n",
    "    train_file_path = f'./F24_Proj3_data/split_{2}/train.csv'\n",
    "    test_file_path = f'./F24_Proj3_data/split_{2}/test.csv'\n",
    "    test_y_file_path = f'./F24_Proj3_data/split_{2}/test_y.csv'\n",
    "\n",
    "    # Load data\n",
    "    X_train = pd.read_csv(train_file_path).iloc[:, 3:]\n",
    "    y_train = pd.read_csv(train_file_path).iloc[:, 1]\n",
    "\n",
    "    X_test = pd.read_csv(test_file_path).iloc[:, 2:]\n",
    "    y_test = pd.read_csv(test_y_file_path).iloc[:, 1]\n",
    "\n",
    "    # LogisticRegressionCV with 'elasticnet' penalty\n",
    "    log_reg_cv = LogisticRegressionCV(\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        l1_ratios=[0.1],  \n",
    "        cv=cv,\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    log_reg_cv.fit(X_train, y_train)\n",
    "\n",
    "    best_C = log_reg_cv.C_\n",
    "    best_l1_ratio = log_reg_cv.l1_ratio_\n",
    "\n",
    "    print(f\"Split {i+1}: Best C: {best_C} | Best l1_ratio: {best_l1_ratio}\")\n",
    "\n",
    "    y_pred_proba = log_reg_cv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f'Split {i+1}: AUC Score for LogisticRegressionCV: {auc_score:.7f} | Execution time: {round(time.time() - start_time, 4)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e264b08-67cc-45f5-815b-d8a25216a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1: AUC Score for LogisticRegression: 0.9869905 | Execution time: 21.9067 seconds\n",
      "Split 2: AUC Score for LogisticRegression: 0.9865600 | Execution time: 21.631 seconds\n",
      "Split 3: AUC Score for LogisticRegression: 0.9862596 | Execution time: 20.7292 seconds\n",
      "Split 4: AUC Score for LogisticRegression: 0.9867768 | Execution time: 21.2396 seconds\n",
      "Split 5: AUC Score for LogisticRegression: 0.9862021 | Execution time: 21.731 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use LogisticRegression with best C and l1 ratio we found in the CV approach\n",
    "num_splits = 5\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    l1_ratio=0.1,  \n",
    "    C=2.7825594,\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for i in range(num_splits):\n",
    "    start_time = time.time()\n",
    "    train_file_path = f'./F24_Proj3_data/split_{i+1}/train.csv'\n",
    "    test_file_path = f'./F24_Proj3_data/split_{i+1}/test.csv'\n",
    "    test_y_file_path = f'./F24_Proj3_data/split_{i+1}/test_y.csv'\n",
    "\n",
    "    # Load data\n",
    "    X_train = pd.read_csv(train_file_path).iloc[:, 3:]\n",
    "    y_train = pd.read_csv(train_file_path).iloc[:, 1]\n",
    "\n",
    "    X_test = pd.read_csv(test_file_path).iloc[:, 2:]\n",
    "    y_test = pd.read_csv(test_y_file_path).iloc[:, 1]\n",
    "\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f'Split {i+1}: AUC Score for LogisticRegression: {auc_score:.7f} | Execution time: {round(time.time() - start_time, 4)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d33c8fb-e007-4363-8890-5180fea79f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = f'./F24_Proj3_data/split_1/train.csv'\n",
    "test_file_path = f'./F24_Proj3_data/split_1/test.csv'\n",
    "test_y_file_path = f'./F24_Proj3_data/split_1/test_y.csv'\n",
    "\n",
    "train = pd.read_csv(train_file_path)\n",
    "test = pd.read_csv(test_file_path)\n",
    "test_y = pd.read_csv(test_y_file_path)\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "             'you', 'your', 'yours', 'their', 'they', 'his', 'her', 'she',\n",
    "             'he', 'a', 'an', 'and', 'is', 'was', 'are', 'were', 'him',\n",
    "             'himself', 'has', 'have', 'it', 'its', 'the', 'us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3693170-9148-4c40-9927-953405794b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review'] = train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "test['review'] = test['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "    stop_words=stop_words,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                        # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer: See Ethan's comment below\n",
    ")\n",
    "\n",
    "dtm_train = vectorizer.fit_transform(train['review'])\n",
    "# dtm_test = vectorizer.fit_transform(test['review'])\n",
    "\n",
    "# log_reg.fit(dtm_train, train['sentiment'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "# preds = log_reg.predict_proba(dtm_test)[:, 1]  # Get the probabilities for class 1\n",
    "\n",
    "# print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cb6294-0591-44e8-a30d-fab08d9e075b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients: -0.2245395020328098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(dtm_train)\n",
    "\n",
    "lasso_model = Lasso(alpha=0.00623)\n",
    "\n",
    "lasso_model.fit(X_train_scaled, train['sentiment'])\n",
    "\n",
    "# Get the coefficients\n",
    "lasso_coefs = lasso_model.coef_\n",
    "print('Lasso Coefficients:', lasso_coefs.sum())\n",
    "\n",
    "# Create a DataFrame with feature names and their coefficients\n",
    "feature_coef_df = pd.DataFrame(\n",
    "    {'Feature': np.array(vectorizer.get_feature_names_out()),\n",
    "     'Coefficient': lasso_coefs})\n",
    "\n",
    "selected_features = feature_coef_df[feature_coef_df['Coefficient'] != 0]\n",
    "vocabulary = list(selected_features['Feature'].values)\n",
    "\n",
    "with open('./myvocab.txt', 'w') as file:\n",
    "    # Iterate through the list and write each word to a new line\n",
    "    for word in vocabulary:\n",
    "        file.write(word + \"\\n\")\n",
    "\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f8dc1b5-0f95-4bc4-8bb5-7b9f8fe5ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                             review  embedding_1  \\\n",
      "21221  16917  Well, I fear that my review of this special wo...     0.015161   \n",
      "1796   42355  What happens when the average joe finds out he...    -0.018924   \n",
      "3875   10466  For years i've had a distant memory of watchin...     0.021243   \n",
      "4276   23582  This film is a brilliant retelling of Shakespe...     0.006077   \n",
      "12025  24055  Mas Oyama was the most successful karate maste...    -0.008232   \n",
      "23248   7451  I'm a huge fan of both Emily Watson (Breaking ...    -0.015999   \n",
      "15017  37586  Alan (Anthony Steffen), an English multi-milli...    -0.022035   \n",
      "23732   5868  This is what Disney Channel shows to kids who ...     0.021406   \n",
      "15807  48453  You'd think that with Ingrid Bergman and Warne...    -0.011025   \n",
      "19237  13120  Someone else called this film a \\fable-horror\\...    -0.014135   \n",
      "\n",
      "       embedding_2  embedding_3  embedding_4  embedding_5  embedding_6  \\\n",
      "21221     0.012336    -0.050433     0.004414    -0.025189     0.039833   \n",
      "1796      0.099957    -0.061771     0.017745    -0.000002    -0.021002   \n",
      "3875      0.065241    -0.028100     0.000646    -0.029823    -0.022392   \n",
      "4276      0.092081    -0.072739     0.003679    -0.000841    -0.014220   \n",
      "12025     0.059877    -0.074810     0.007327     0.006104    -0.003420   \n",
      "23248     0.047191    -0.054182     0.003721    -0.013257    -0.032321   \n",
      "15017     0.088564    -0.044070    -0.043991    -0.013827     0.003985   \n",
      "23732     0.051248    -0.057412     0.024618    -0.008548    -0.029203   \n",
      "15807     0.058658    -0.017032     0.001130    -0.056710    -0.007630   \n",
      "19237     0.068334    -0.079045    -0.021049    -0.021654    -0.006939   \n",
      "\n",
      "       embedding_7  embedding_8  ...  embedding_1527  embedding_1528  \\\n",
      "21221     0.030433     0.031047  ...        0.017378        0.048564   \n",
      "1796     -0.008360     0.026913  ...       -0.004780        0.000651   \n",
      "3875      0.014412     0.028374  ...        0.031372       -0.001464   \n",
      "4276      0.016170     0.021603  ...       -0.013148        0.004075   \n",
      "12025    -0.007406     0.054964  ...        0.003976       -0.006579   \n",
      "23248     0.011710    -0.010346  ...        0.019777        0.042458   \n",
      "15017     0.061334    -0.002378  ...       -0.002824       -0.010350   \n",
      "23732     0.029464     0.000357  ...       -0.006380       -0.006631   \n",
      "15807     0.012189    -0.004146  ...        0.011181       -0.002273   \n",
      "19237     0.019787     0.036664  ...       -0.037746       -0.004805   \n",
      "\n",
      "       embedding_1529  embedding_1530  embedding_1531  embedding_1532  \\\n",
      "21221       -0.000815       -0.004250       -0.005809       -0.006559   \n",
      "1796        -0.013990        0.005724        0.009638        0.018981   \n",
      "3875         0.004649       -0.018159       -0.016348        0.031372   \n",
      "4276         0.028336       -0.021746        0.015273        0.011712   \n",
      "12025        0.001009       -0.016331       -0.014276        0.017365   \n",
      "23248        0.003282       -0.001771        0.012537        0.005334   \n",
      "15017       -0.011969       -0.025181       -0.003328       -0.013185   \n",
      "23732       -0.031971        0.003755       -0.001248        0.014980   \n",
      "15807        0.037743       -0.009057       -0.018547        0.019264   \n",
      "19237       -0.000698        0.010312       -0.001946        0.012558   \n",
      "\n",
      "       embedding_1533  embedding_1534  embedding_1535  embedding_1536  \n",
      "21221       -0.017225        0.023376        0.004352       -0.041870  \n",
      "1796        -0.005229        0.004524       -0.026814       -0.007981  \n",
      "3875         0.004334        0.008630        0.033845       -0.007556  \n",
      "4276         0.003402        0.056621        0.036318       -0.035668  \n",
      "12025        0.010470       -0.010087        0.003435       -0.033830  \n",
      "23248        0.004413       -0.021700        0.013082       -0.001808  \n",
      "15017        0.017356        0.003906        0.005743       -0.024084  \n",
      "23732        0.012871        0.018010        0.020047        0.009547  \n",
      "15807        0.021943        0.000681        0.003707       -0.022781  \n",
      "19237       -0.008928        0.019169       -0.001174       -0.020547  \n",
      "\n",
      "[10 rows x 1538 columns]\n",
      "Well, I fear that my review of this special won't heed much different observation than the others before me, but I literally just watched it- during a PBS membership drive- and frankly I'm too excited NOT to say anything. To really appreciate the enigma that is Barbra Streisand, you have to look back before the movies. Before the Broadway phenomenon of the mid-60's. When television was still a young medium, there was a form of entertainment very prominent on the air that is but a memory today: musical variety. Some musical shows were weekly series, but others were single, one-time specials, usually showcasing the special talent of the individual performer. This is where we get the raw, uninhibited first looks at Streisand. She had already been a guest performer on other variety shows including Garry Moore, Ed Sullivan, and scored a major coup in a one-time only tandem appearance with the woman who would pass her the baton of belter extraordinary: Judy Garland. In 1966, COLOR ME BARBRA introduced Barbra Streisand in color (hence the title), but copied the format of her first special a year earlier almost to the letter. In 3 distinct acts, we get an abstract Streisand (in an after-hours art museum looking at and sometimes becoming the works of art), a comic Streisand working an already adoring audience in a studio circus (populated with many fuzzy and furry animals), and best of all, a singing Streisand in mini-concert format just-- well, frankly, just doing it. <br /><br />It amazes me that she still had the film debut of FUNNY GIRL yet to come, as well as turns as songwriter, director, and political activist. Here, she is barely 24 years old, doing extraordinary things because, as she puts it in her own on-camera introduction, 'we didn't know we couldn't, so we did.' The art museum sequence is shot in Philadelphia over one weekend immediately after the museum closed to the public on Saturday evening, and apparently done with only ONE color camera. Yet there are cuts, dissolves, and tracking shots galore, resulting in one rather spectacular peak moment-- the modern, slightly beatnik-flavored, \\Gotta Move.\\\" After getting lost amongst the modern abstracts, jazz-club bongos begin, with Streisand emerging in a psychedelic gown and glittering eye makeup, doing the catchy staccato tune with almost androgynous sex appeal. It is not until Act 3, believe it or not, that the moment is matched or bettered by another feat: in the concert sequence, in a white gown and pearl earrings, Streisand recites the torchy \\\"Any Place I Hang My Hat is Home,\\\" tearing into the final notes and revealing one of those climactic belts that makes you scream like a little girl even if you're 44 years old...and a guy. Just plain old great television. Check it out.\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     explanation\u001b[38;5;241m.\u001b[39mshow_in_notebook(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m selected_reviews\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 22\u001b[0m     explain_review(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[35], line 17\u001b[0m, in \u001b[0;36mexplain_review\u001b[0;34m(review_text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplain_review\u001b[39m(review_text):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(review_text)\n\u001b[0;32m---> 17\u001b[0m     explanation \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(review_text, log_reg\u001b[38;5;241m.\u001b[39mpredict_proba(dtm_test)[:, \u001b[38;5;241m1\u001b[39m], num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     19\u001b[0m     explanation\u001b[38;5;241m.\u001b[39mshow_in_notebook(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lime/lime_text.py:413\u001b[0m, in \u001b[0;36mLimeTextExplainer.explain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    406\u001b[0m indexed_string \u001b[38;5;241m=\u001b[39m (IndexedCharacters(\n\u001b[1;32m    407\u001b[0m     text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow, mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string)\n\u001b[1;32m    408\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_level \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    409\u001b[0m                   IndexedString(text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow,\n\u001b[1;32m    410\u001b[0m                                 split_expression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_expression,\n\u001b[1;32m    411\u001b[0m                                 mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string))\n\u001b[1;32m    412\u001b[0m domain_mapper \u001b[38;5;241m=\u001b[39m TextDomainMapper(indexed_string)\n\u001b[0;32m--> 413\u001b[0m data, yss, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_labels_distances(\n\u001b[1;32m    414\u001b[0m     indexed_string, classifier_fn, num_samples,\n\u001b[1;32m    415\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(yss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/lime/lime_text.py:482\u001b[0m, in \u001b[0;36mLimeTextExplainer.__data_labels_distances\u001b[0;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    480\u001b[0m     data[i, inactive] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    481\u001b[0m     inverse_data\u001b[38;5;241m.\u001b[39mappend(indexed_string\u001b[38;5;241m.\u001b[39minverse_removing(inactive))\n\u001b[0;32m--> 482\u001b[0m labels \u001b[38;5;241m=\u001b[39m classifier_fn(inverse_data)\n\u001b[1;32m    483\u001b[0m distances \u001b[38;5;241m=\u001b[39m distance_fn(sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix(data))\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, labels, distances\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "myvocab = open(\"myvocab.txt\", \"r\").read().splitlines()\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=myvocab, ngram_range=(1, 4))\n",
    "dtm_test = tfidf_vectorizer.fit_transform(dtm_train['review'])\n",
    "# log_reg.fit(dtm_test, test_y['sentiment'])\n",
    "# print(myvocab)\n",
    "# print(test_y)\n",
    "# print(dtm_test)\n",
    "positive_reviews = test[test_y['sentiment'] == 1].sample(5, random_state=42)\n",
    "negative_reviews = test[test_y['sentiment'] == 0].sample(5, random_state=42)\n",
    "selected_reviews = pd.concat([positive_reviews, negative_reviews])\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=['Negative', 'Positive'])\n",
    "\n",
    "def explain_review(review_text):\n",
    "    print(review_text)\n",
    "    explanation = explainer.explain_instance(review_text, log_reg.predict_proba, num_features=10)\n",
    "    \n",
    "    explanation.show_in_notebook(text=True)\n",
    "\n",
    "for i, row in selected_reviews.iterrows():\n",
    "    explain_review(row['review'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee849d-ce94-4259-b815-b31207144551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
